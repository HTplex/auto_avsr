{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556076dd-8537-4356-8ac3-f4a838b2e896",
   "metadata": {},
   "source": [
    "# AUDIO/VISUAL SPEECH RECOGNITION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27269bb4-af6f-4d93-9141-799400b9131e",
   "metadata": {},
   "source": [
    "This tutorial shows how to use our `InferencePipeline` to perform audio or visual speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e067f7d-cc60-43a0-b65a-7272c9cecc81",
   "metadata": {},
   "source": [
    "**Note** This tutorial requires `mediapipe` or `retinaface` detector. Please refer to [preparation](../preparation#setup) for installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97575fc-bb77-43d0-8eb7-f94baf4cdc89",
   "metadata": {},
   "source": [
    "**Note** To run this tutorial, please make sure you are in tutorials folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa81fb4f-6e89-4238-9f47-3f66eedd3c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2fb0de3-af7f-477e-baef-dd595effeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875bb66-4e96-4c51-85d6-a129748d197a",
   "metadata": {},
   "source": [
    "## 1. Build an inference pipeline\n",
    "\n",
    "The InferencePipeline carries out the following three steps:\n",
    "\n",
    "1. Load audio or video data\n",
    "2. Run pre-processing functions\n",
    "3. Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebfe406-2cd8-4b1a-bf62-32ce1934fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agent_h/miniconda3/envs/autoavsr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from lightning import ModelModule\n",
    "from datamodule.transforms import AudioTransform, VideoTransform\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da3a91c-f515-45f5-8172-ababfd786596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, _ = parser.parse_known_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac03a1e1-3082-43ca-bb44-e58f96464bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipeline(torch.nn.Module):\n",
    "    def __init__(self, args, ckpt_path,lm_path, detector=\"retinaface\"):\n",
    "        super(InferencePipeline, self).__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            lm_path,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(lm_path)\n",
    "        self.modality = args.modality\n",
    "        if self.modality == \"audio\":\n",
    "            self.audio_transform = AudioTransform(subset=\"test\")\n",
    "        elif self.modality == \"video\":\n",
    "            if detector == \"mediapipe\":\n",
    "                from preparation.detectors.mediapipe.detector import LandmarksDetector\n",
    "                from preparation.detectors.mediapipe.video_process import VideoProcess\n",
    "                self.landmarks_detector = LandmarksDetector()\n",
    "                self.video_process = VideoProcess(convert_gray=False)\n",
    "            elif detector == \"retinaface\":\n",
    "                from preparation.detectors.retinaface.detector import LandmarksDetector\n",
    "                from preparation.detectors.retinaface.video_process import VideoProcess\n",
    "                self.landmarks_detector = LandmarksDetector(device=\"cuda:0\")\n",
    "                self.video_process = VideoProcess(convert_gray=False)\n",
    "            self.video_transform = VideoTransform(subset=\"test\")\n",
    "\n",
    "        ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
    "        self.modelmodule = ModelModule(args)\n",
    "        self.modelmodule.model.load_state_dict(ckpt)\n",
    "        self.modelmodule.eval()\n",
    "\n",
    "    def load_video(self, data_filename):\n",
    "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
    "\n",
    "    def forward(self, data_filename):\n",
    "        data_filename = os.path.abspath(data_filename)\n",
    "        assert os.path.isfile(data_filename), f\"data_filename: {data_filename} does not exist.\"\n",
    "\n",
    "\n",
    "        if self.modality == \"video\":\n",
    "            video = self.load_video(data_filename)\n",
    "            landmarks = self.landmarks_detector(video)\n",
    "            video = self.video_process(video, landmarks)\n",
    "            video = torch.tensor(video)\n",
    "            video = video.permute((0, 3, 1, 2))\n",
    "            video = self.video_transform(video)\n",
    "            with torch.no_grad():\n",
    "                transcript = self.modelmodule(video)\n",
    "            prompt = f\"通过语境将语音识别的拼音识别成通顺的中文，并加上标点：{transcript.lower()}\"\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            generated_ids = self.model.generate(\n",
    "                model_inputs.input_ids,\n",
    "                max_new_tokens=512\n",
    "            )\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            \n",
    "            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        return response\n",
    "\n",
    "    def load_audio(self, data_filename):\n",
    "        waveform, sample_rate = torchaudio.load(data_filename, normalize=True)\n",
    "        return waveform, sample_rate\n",
    "\n",
    "    def load_video(self, data_filename):\n",
    "        return torchvision.io.read_video(data_filename, pts_unit=\"sec\")[0].numpy()\n",
    "\n",
    "    def audio_process(self, waveform, sample_rate, target_sample_rate=16000):\n",
    "        if sample_rate != target_sample_rate:\n",
    "            waveform = torchaudio.functional.resample(\n",
    "                waveform, sample_rate, target_sample_rate\n",
    "            )\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "401c84d6-5910-4959-8761-34cbd1d18c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████| 11/11 [00:07<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "data_filename = \"/home/agent_h/data/datasets/autoavsr_bili_splitted_sampled/lrs3/lrs3_video_seg16s/test/202308071746_bilibili_xiaochanwan_008_000085/202308071746_bilibili_xiaochanwan_008_000085.mp4\"\n",
    "setattr(args, 'modality', 'video')\n",
    "model_path = \"/home/agent_h/data/ckpts/0204_0217_autoavsr_cnv2/epoch=65.pth\"\n",
    "lm_path = \"/home/agent_h/data/llms/Qwen2.5-72B-Instruct-AWQ\"\n",
    "pipeline = InferencePipeline(args, model_path,lm_path, detector=\"retinaface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0fe308-6a0f-4c57-869f-a0114bebdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尊重自己，比大而有爱或得人的声好。那今天这期视频到这就结束了，大家看完不要忘记一键三连感谢，这样的话不要忘记关注。大家拜拜。\n"
     ]
    }
   ],
   "source": [
    "transcript = pipeline(data_filename)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f84d2b-6fdb-4132-b099-43ce8e70116a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
